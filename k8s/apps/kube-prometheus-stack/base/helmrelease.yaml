---
# yaml-language-server: $schema=https://kube-schemas.pages.dev/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
spec:
  chartRef:
    kind: OCIRepository
    name: kube-prometheus-stack
  interval: 1h
  values:
    # Ensure CRDs are installed by the chart (you can also pre-install separately)
    prometheusOperator:
      crds:
        enabled: true
        upgradeJob:
          enabled: true
      # Keep the operator itself modest
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
        limits:
          cpu: 500m
          memory: 600Mi
      admissionWebhooks:
        patch:
          enabled: true
    # Reduce noisy rules you likely canâ€™t scrape on Talos without extra config.
    # Keep core Kubernetes and node rules enabled.
    defaultRules:
      create: true
      rules:
        alertmanager: true
        etcd: false # Talos etcd metrics usually not exposed by default
        configReloaders: true
        general: true
        kubeApiserver: true
        kubeApiserverAvailability: true
        kubeApiserverSlos: true
        kubeApiserverError: true
        kubeControllerManager: false # Disable unless you explicitly expose metrics
        kubeDns: true
        kubeEtcd: false # Same as etcd
        kubelet: true
        kubeProxy: false # With Cilium (kube-proxy replacement), disable
        kubeScheduler: false # Disable unless you expose metrics
        kubeStateMetrics: true
        kubeletMetrics: true
        kubeletRules: true
        node: true
        prometheus: true
        prometheusOperator: true
        time: true
    # Disable components that require control-plane metrics endpoints you may not expose
    kubeProxy:
      enabled: false
    kubeEtcd:
      enabled: false
    kubeControllerManager:
      enabled: false
    kubeScheduler:
      enabled: false
    # ---------------------------
    # Prometheus server
    # ---------------------------
    prometheus:
      enabled: true
      route:
        main:
          enabled: true
          hostnames: ["prometheus.${subdomain}.ipreston.net"]
          parentRefs:
            - name: envoy-internal
              namespace: envoy-gateway
              sectionName: https
      prometheusSpec:
        replicas: 1
        logLevel: info
        scrapeInterval: 30s
        evaluationInterval: 30s
        enableAdminAPI: false
        walCompression: true
        externalLabels:
          cluster: dev
        # Let Prometheus discover ServiceMonitors/PodMonitors & Rules from ALL namespaces.
        # This is key so it can pick up Cilium/Longhorn monitors installed by their charts.
        serviceMonitorSelector: {}
        serviceMonitorNamespaceSelector: {}
        podMonitorSelector: {}
        podMonitorNamespaceSelector: {}
        ruleSelector: {}
        ruleNamespaceSelector: {}
        # Storage/retention tuned for homelab (adjust to taste)
        retention: 30d
        retentionSize: "10GB"
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: longhorn-default
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 50Gi
        # Avoid runaway cardinality on small clusters
        # (safe defaults; tweak if you hit TSDB limits)
        scrapeConfigSelector: {}
        scrapeConfigNamespaceSelector: {}
        ignoreNamespaceSelectors: false
        allowOverlappingBlocks: false
        queryLogFile: ""
        enableFeatures: []
        resources:
          requests:
            cpu: 500m
            memory: 1.5Gi
          limits:
            cpu: "2"
            memory: 4Gi
    # ---------------------------
    # Alertmanager
    # ---------------------------
    alertmanager:
      enabled: true
      route:
        main:
          enabled: true
          hostnames: ["alertmanager.${subdomain}.ipreston.net"]
          parentRefs:
            - name: envoy-internal
              namespace: envoy-gateway
              sectionName: https
      alertmanagerSpec:
        replicas: 1
        retention: 120h
        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: longhorn-default
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 10Gi
        resources:
          requests:
            cpu: 100m
            memory: 200Mi
          limits:
            cpu: 500m
            memory: 600Mi
    # ---------------------------
    # Grafana
    # ---------------------------
    grafana:
      enabled: false
      forceDeployDashboards: true
      operator:
        dashboardsConfigMapRefEnabled: true
        folder: observability
        matchLabels:
          grafana.internal/instance: grafana
      # persistence:
      #   enabled: true
      #   type: sts
      #   storageClassName: longhorn-default
      #   accessModes:
      #     - ReadWriteOnce
      #   size: 10Gi
      # Automatically pick up dashboards/datasources from ConfigMaps across all namespaces
      sidecar:
        dashboards:
          enabled: true
          label: grafana_dashboard
          labelValue: "1"
          searchNamespace: ALL
          folder: /var/lib/grafana/dashboards
          provider:
            foldersFromFilesStructure: true
        datasources:
          enabled: true
          label: grafana_datasource
          labelValue: "1"
          searchNamespace: ALL
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
        limits:
          cpu: 500m
          memory: 600Mi
    # ---------------------------
    # Node exporter (daemonset)
    # ---------------------------
    prometheus-node-exporter:
      enabled: true
      # Run on all nodes, including Talos control-planes (tainted)
      tolerations:
        - key: "node-role.kubernetes.io/control-plane"
          operator: "Exists"
          effect: "NoSchedule"
        - key: "node-role.kubernetes.io/master"
          operator: "Exists"
          effect: "NoSchedule"
      resources:
        requests:
          cpu: 30m
          memory: 64Mi
        limits:
          cpu: 200m
          memory: 200Mi
    # ---------------------------
    # kube-state-metrics
    # ---------------------------
    kube-state-metrics:
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
        limits:
          cpu: 500m
          memory: 600Mi
