# yaml-language-server: $schema=https://kube-schemas.pages.dev/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
spec:
  chartRef:
    kind: OCIRepository
    name: kube-prometheus-stack
  interval: 1h
  values:
    # Ensure CRDs are installed by the chart (you can also pre-install separately)
    prometheusOperator:
      crds:
        enabled: true
      # Keep the operator itself modest
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
        limits:
          cpu: 500m
          memory: 600Mi
      admissionWebhooks:
        patch:
          enabled: true
    # Reduce noisy rules you likely canâ€™t scrape on Talos without extra config.
    # Keep core Kubernetes and node rules enabled.
    defaultRules:
      create: true
      rules:
        alertmanager: true
        etcd: false # Talos etcd metrics usually not exposed by default
        configReloaders: true
        general: true
        kubeApiserver: true
        kubeApiserverAvailability: true
        kubeApiserverSlos: true
        kubeApiserverError: true
        kubeControllerManager: false # Disable unless you explicitly expose metrics
        kubeDns: true
        kubeEtcd: false # Same as etcd
        kubelet: true
        kubeProxy: false # With Cilium (kube-proxy replacement), disable
        kubeScheduler: false # Disable unless you expose metrics
        kubeStateMetrics: true
        kubeletMetrics: true
        kubeletRules: true
        node: true
        prometheus: true
        prometheusOperator: true
        time: true
    # Disable components that require control-plane metrics endpoints you may not expose
    kubeProxy:
      enabled: false
    kubeEtcd:
      enabled: false
    kubeControllerManager:
      enabled: false
    kubeScheduler:
      enabled: false
    # ---------------------------
    # Prometheus server
    # ---------------------------
    prometheus:
      enabled: true
      ingress:
        enabled: false # Usually keep Prometheus UI internal; use kubectl port-forward when needed
      prometheusSpec:
        replicas: 1
        logLevel: info
        scrapeInterval: 30s
        evaluationInterval: 30s
        enableAdminAPI: false
        walCompression: true
        externalLabels:
          cluster: dev
        # Let Prometheus discover ServiceMonitors/PodMonitors & Rules from ALL namespaces.
        # This is key so it can pick up Cilium/Traefik/Longhorn monitors installed by their charts.
        serviceMonitorSelector: {}
        serviceMonitorNamespaceSelector: {}
        podMonitorSelector: {}
        podMonitorNamespaceSelector: {}
        ruleSelector: {}
        ruleNamespaceSelector: {}
        # Storage/retention tuned for homelab (adjust to taste)
        retention: 30d
        retentionSize: "10GB"
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: longhorn-default
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 50Gi
        # Avoid runaway cardinality on small clusters
        # (safe defaults; tweak if you hit TSDB limits)
        scrapeConfigSelector: {}
        scrapeConfigNamespaceSelector: {}
        ignoreNamespaceSelectors: false
        allowOverlappingBlocks: false
        queryLogFile: ""
        enableFeatures: []
        additionalArgs:
          - --web.enable-lifecycle
          - --storage.tsdb.wal-compression
          - --query.max-concurrency=8
          - --query.max-samples=10000000
        resources:
          requests:
            cpu: 500m
            memory: 1.5Gi
          limits:
            cpu: "2"
            memory: 4Gi
    # ---------------------------
    # Alertmanager
    # ---------------------------
    alertmanager:
      enabled: true
      alertmanagerSpec:
        replicas: 1
        retention: 120h
        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: longhorn-default
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 10Gi
        resources:
          requests:
            cpu: 100m
            memory: 200Mi
          limits:
            cpu: 500m
            memory: 600Mi
      # Public access to Alertmanager (optional). Set your real domain + cluster issuer.
      ingress:
        enabled: true
        ingressClassName: traefik
        annotations:
          cert-manager.io/cluster-issuer: "letsencrypt-prod"
        hosts:
          - alertmanager.${subdomain}.ipreston.net
        tls:
          - secretName: alertmanager-tls
            hosts:
              - alertmanager.${subdomain}.ipreston.net
    # ---------------------------
    # Grafana
    # ---------------------------
    grafana:
      enabled: true
      adminUser: admin
      adminPassword: "CHANGE_ME"
      service:
        type: ClusterIP
      ingress:
        enabled: true
        ingressClassName: traefik
        annotations:
          cert-manager.io/cluster-issuer: "letsencrypt-prod"
        hosts:
          - grafana.${subdomain}.ipreston.net
        tls:
          - secretName: grafana-tls
            hosts:
              - grafana.${subdomain}.ipreston.net
      env:
        GF_SERVER_ROOT_URL: "https://grafana.${subdomain}.ipreston.net"
        GF_SECURITY_ADMIN_PASSWORD: "CHANGE_ME"
      persistence:
        enabled: true
        type: pvc
        storageClassName: longhorn-default
        accessModes:
          - ReadWriteOnce
        size: 10Gi
      # Automatically pick up dashboards/datasources from ConfigMaps across all namespaces
      sidecar:
        dashboards:
          enabled: true
          label: grafana_dashboard
          labelValue: "1"
          searchNamespace: ALL
          folder: /var/lib/grafana/dashboards
          provider:
            foldersFromFilesStructure: true
        datasources:
          enabled: true
          label: grafana_datasource
          labelValue: "1"
          searchNamespace: ALL
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
        limits:
          cpu: 500m
          memory: 600Mi
    # ---------------------------
    # Node exporter (daemonset)
    # ---------------------------
    prometheus-node-exporter:
      enabled: true
      # Run on all nodes, including Talos control-planes (tainted)
      tolerations:
        - key: "node-role.kubernetes.io/control-plane"
          operator: "Exists"
          effect: "NoSchedule"
        - key: "node-role.kubernetes.io/master"
          operator: "Exists"
          effect: "NoSchedule"
      resources:
        requests:
          cpu: 30m
          memory: 64Mi
        limits:
          cpu: 200m
          memory: 200Mi
    # ---------------------------
    # kube-state-metrics
    # ---------------------------
    kube-state-metrics:
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
        limits:
          cpu: 500m
          memory: 600Mi
